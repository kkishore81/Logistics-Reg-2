{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa98d22-2b7a-4dd4-a419-3b7e6c7d9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245c405-faa3-4d32-bcf5-e093a01c48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search CV is used to find the best combination of hyperparameters for a machine learning model. It works by exhaustively searching through a predefined set of hyperparameters \n",
    "#and cross-validating the model's performance for each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b992b03-5175-49db-b5e9-c07417f38279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867be63-4085-4648-b3eb-87b22285bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search CV explores all possible hyperparameter combinations within a predefined search space, while random search CV randomly samples hyperparameters from the search space.\n",
    "#Grid search is suitable for small search spaces, while random search is more efficient for larger spaces or when you have limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76cf157-2a93-4182-8030-a29279ef6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31042e80-af16-4a6d-ae61-e9ce5c7161f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data leakage occurs when information from the test or validation dataset is inadvertently used during model training, leading to overly optimistic performance estimates. \n",
    "#An example is using future information as a predictor, such as stock prices, when predicting historical stock trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06886624-bd43-49bc-a76c-cd4192b21933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b6d6e-237c-4fec-805a-de345427b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prevent data leakage, we should ensure that the model only uses information that would be available at the time of prediction. \n",
    "#Techniques include proper data splitting, feature engineering, and careful preprocessing to avoid including future or unintended information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f5a78-2d1c-46f2-aeba-f02a2002fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4de2f-e8d9-4f4c-ac29-e0d891f65aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A confusion matrix is a table that shows the true positive, true negative, false positive, and false negative counts for a classification model. \n",
    "#It provides insights into how well the model is performing in terms of classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b93af3-5d34-4e55-ac6e-8bd474375d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2324f-12b9-4391-990a-12c44e952bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positives among all actual positives.\n",
    "#Precision focuses on the accuracy of positive predictions, while recall assesses the model's ability to find all actual positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfdb03-7967-4695-82f4-48ea7033970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfa5fc-9a47-4468-baca-c6798c7bbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By examining the confusion matrix, you can identify the types of errors your model is making. \n",
    "#False positives indicate instances where the model incorrectly predicted a positive class, \n",
    "#while false negatives indicate instances where it incorrectly predicted a negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5834d-b292-427a-8253-7008240b7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b388fe1-5f8d-485b-af4a-920135ce48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common metrics derived from a confusion matrix include accuracy, precision, recall, F1-score, and specificity.\n",
    "#They are calculated using combinations of true positive, true negative, false positive, and false negative counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a73b3-0945-45b5-8dc0-70a6d1537013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3dd53-78c4-42c1-a94e-7a5cd2df0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy is the ratio of correct predictions (true positives and true negatives) to the total number of predictions. \n",
    "#It's an overall measure of model performance but can be misleading in the presence of imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba28ce-478c-42ad-867a-81b4f8bde8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62447070-b2fa-418b-9bef-99b8a39231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A confusion matrix can help identify biases or limitations by showing disparities in model performance across different classes. \n",
    "#If certain classes consistently have lower precision or recall, it may indicate biases or issues with data quality or model training for those classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
